{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 1: Extraction des tweets \n",
    "``REMARQUE: Le code source de cette étape doit étre executer si seulement vous aimerai traiter le code from Scratch``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour faire du machine learning sur des tweets il faut des tweets. \n",
    "\n",
    "Pour cela on va utiliser l’API que Twitter lui-même propose. \n",
    "\n",
    "Elle permet de récupérer des tweets en ajoutant certaines conditions sur le type de tweets que nous souhaitons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J’ai choisi de récupérer les tweets sur trois fois qui contiennent les ***news*** ou ***actualités***\n",
    "\n",
    "Les tweets doivent être en français et anglais \n",
    "\n",
    "voici comment récupérer des tweets avec **Tweepy**: \n",
    "\n",
    "1.\tD’abord nous aurons besoin d’un compte Twitter développeur. \n",
    "2.\tUne fois notre compte développeur crée, il vous faudra quelques informations : l’API key, l’API secret key, l’Access token, l’Access token secret. \n",
    "3.\tEnfin, on peut extraire  les tweets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYVbOH6Y-aid",
    "outputId": "c45dc1ac-0964-4acb-cbe6-a9cdc9888b08"
   },
   "source": [
    "**`!pip install python-twitter`**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1   Authetifacation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "93uEEfGV-aif"
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "\n",
    "api = twitter.Api(consumer_key=\"FHolBw3mVFGLtbfLb1m8zGVFV\",\n",
    "                  consumer_secret=\"dIT8PT37qcpZkbaV1gnHfCiSHq60B9OmJnfxGQwwGyHacQ2n2T\",\n",
    "                  access_token_key=\"1334257369027579907-CrBHS22plIxU4wpSHaRR1wtZdNuKYv\",\n",
    "                  access_token_secret=\"Mga6h9zLXCZB53CPtDmbEAzprF8saZSI44Fis9ZfV7ooI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GONgt9EM-aig",
    "outputId": "f675e60f-c635-476a-9ce6-840aedaf6cf2",
    "scrolled": false
   },
   "source": [
    "**`!pip install tweepy`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "h_MsPWI9-aih"
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "auth = tweepy.OAuthHandler(\"FHolBw3mVFGLtbfLb1m8zGVFV\", \"dIT8PT37qcpZkbaV1gnHfCiSHq60B9OmJnfxGQwwGyHacQ2n2T\")\n",
    "auth.set_access_token(\"1334257369027579907-CrBHS22plIxU4wpSHaRR1wtZdNuKYv\", \"Mga6h9zLXCZB53CPtDmbEAzprF8saZSI44Fis9ZfV7ooI\")\n",
    "\n",
    "api = tweepy.API(auth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2   Extraction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oSj_PgIi-aih"
   },
   "outputs": [],
   "source": [
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "search_words = \"news OR actualité\"\n",
    "new_search = search_words + \" -filter:retweets\"\n",
    "date_since = \"2020-12-08\"\n",
    "date_until = \"\"\n",
    "totalTweets =2000\n",
    "count = 1000\n",
    "lang = \"fr\" or \"en\"\n",
    "geocode = \"\"\n",
    "result_type = \"recent\"\n",
    "include_entities = True\n",
    "filename = \"testT3witter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9hWvNnvE-aih"
   },
   "outputs": [],
   "source": [
    "# Fonction de gestion de la pagination dans notre recherche\n",
    "def limit_handled(cursor):\n",
    "    while True:\n",
    "        try:\n",
    "            yield cursor.next()\n",
    "        except tweepy.RateLimitError:\n",
    "            print('Reached rate limite. Sleeping for >15 minutes')\n",
    "            time.sleep(15 * 61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bbP3fwVA-aih"
   },
   "outputs": [],
   "source": [
    "# Fonction pour effectuer la recherche à l'aide de l'API Twitter\n",
    "def search_tweets(new_search, date_since):\n",
    "\n",
    "    # effectue la recherche à l'aide des variables définies\n",
    "    for tweet in limit_handled(tweepy.Cursor(api.search,\n",
    "                               q=new_search,\n",
    "                               count=count,\n",
    "                               tweet_mode='extended',\n",
    "                               lang=lang,\n",
    "                               geocode=geocode,\n",
    "                               result_type=result_type,\n",
    "                               include_entities=include_entities,\n",
    "                               since=date_since,\n",
    "                               until=date_until).items(totalTweets)):\n",
    "\n",
    "        try:\n",
    "\n",
    "            # Vérifie s'il s'agit d'un tweet étendu (> 140 caractères)\n",
    "            content = tweet.full_text\n",
    "\n",
    "            '''Convertir toutes les références de caractères nommés et numériques\n",
    "             (par exemple & gt ;, & # 62 ;, & # x3e;) dans la chaîne s au\n",
    "             caractères Unicode correspondants'''\n",
    "            content = (content.replace('&amp;', '&').replace('&lt;', '<')\n",
    "                       .replace('&gt;', '>').replace('&quot;', '\"')\n",
    "                       .replace('&#39;', \"'\").replace(';', \" \")\n",
    "                       .replace(r'\\u', \" \").replace('\\u2026', \"\"))\n",
    "\n",
    "            # Enregistrer d'autres informations du tweet\n",
    "            user = tweet.author.screen_name\n",
    "            timeTweet = tweet.created_at\n",
    "            source = tweet.source\n",
    "            tweetId = tweet.id\n",
    "            tweetUrl = \"https://twitter.com/statuses/\" + str(tweetId)\n",
    "\n",
    "            # Exclure les retweets, trop de mentions et trop de hashtags\n",
    "            if not any((('RT @' in content, 'RT' in content,\n",
    "                       content.count('@') >= 2, content.count('#') >= 3))):\n",
    "\n",
    "                # Enregistre les informations du tweet dans une nouvelle ligne du fichier CSV\n",
    "                writer.writerow([content, timeTweet,\n",
    "                                user, source, tweetId, tweetUrl])\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Encountered Exception:', e)\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 797090,
     "status": "ok",
     "timestamp": 1607434241263,
     "user": {
      "displayName": "Amina Sridi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfTzr62TA1NJlnTLYck_lihI2hy50RTcOM1aYznQ=s64",
      "userId": "03842942722047736776"
     },
     "user_tz": -60
    },
    "id": "F38KSdH5-aih",
    "outputId": "0288846a-d6dc-44c4-cb7f-c6d25f1b8a5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport csv\\nfrom requests.exceptions import Timeout, ConnectionError\\nimport ssl\\nfrom requests.packages.urllib3.exceptions import ReadTimeoutError\\ndef work():\\n\\n    # Ouverture d\\'un fichier CSV pour enregistrer les tweets rassemblés\\n    with open(filename+\".csv\", \\'w\\') as file:\\n        global writer\\n        writer = csv.writer(file)\\n\\n        # Ajouter une ligne d\\'en-tête au CSV\\n        writer.writerow([\"Contenu_\", \"Date_\", \"Utilisateur_\",\\n                         \"Source_\", \"ID_\", \"Tweet URL\"])\\n\\n        # Initialisation de la recherche Twitter\\n        try:\\n            search_tweets(search_words, date_since)\\n\\n        # Arrêtez temporairement lorsque vous atteignez la limite de taux Twitter\\n        except tweepy.RateLimitError:\\n            print(\"RateLimitError...waiting ~15 minutes to continue\")\\n            time.sleep(1001)\\n            search_tweets(search_words, date_since)\\n\\n        # Arrêtez temporairement en cas de dépassement de délai ou d\\'erreur de connexion\\n        except (Timeout, ssl.SSLError, ReadTimeoutError,\\n                ConnectionError) as exc:\\n            print(\"Timeout/connection error...waiting ~15 minutes to continue\")\\n            time.sleep(1001)\\n            search_tweets(search_words, date_since)\\n\\n        # Arrêtez temporairement lorsque vous recevez d\\'autres erreurs\\n        except tweepy.TweepError as e:\\n            if \\'Failed to send request:\\' in e.reason:\\n                print(\"Time out error caught.\")\\n                time.sleep(1001)\\n                search_tweets(search_words, date_since)\\n            elif\\'Too Many Requests\\' in e.reason:\\n                print(\"Too many requests, sleeping for 15 min\")\\n                time.sleep(1001)\\n                search_tweets(search_words, date_since)\\n            else:\\n                print(e)\\n                print(\"Other error with this user...passing\")\\n                pass\\n\\n\\nif __name__ == \\'__main__\\':\\n\\n    work()\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import csv\n",
    "from requests.exceptions import Timeout, ConnectionError\n",
    "import ssl\n",
    "from requests.packages.urllib3.exceptions import ReadTimeoutError\n",
    "def work():\n",
    "\n",
    "    # Ouverture d'un fichier CSV pour enregistrer les tweets rassemblés\n",
    "    with open(filename+\".csv\", 'w') as file:\n",
    "        global writer\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Ajouter une ligne d'en-tête au CSV\n",
    "        writer.writerow([\"Contenu_\", \"Date_\", \"Utilisateur_\",\n",
    "                         \"Source_\", \"ID_\", \"Tweet URL\"])\n",
    "\n",
    "        # Initialisation de la recherche Twitter\n",
    "        try:\n",
    "            search_tweets(search_words, date_since)\n",
    "\n",
    "        # Arrêtez temporairement lorsque vous atteignez la limite de taux Twitter\n",
    "        except tweepy.RateLimitError:\n",
    "            print(\"RateLimitError...waiting ~15 minutes to continue\")\n",
    "            time.sleep(1001)\n",
    "            search_tweets(search_words, date_since)\n",
    "\n",
    "        # Arrêtez temporairement en cas de dépassement de délai ou d'erreur de connexion\n",
    "        except (Timeout, ssl.SSLError, ReadTimeoutError,\n",
    "                ConnectionError) as exc:\n",
    "            print(\"Timeout/connection error...waiting ~15 minutes to continue\")\n",
    "            time.sleep(1001)\n",
    "            search_tweets(search_words, date_since)\n",
    "\n",
    "        # Arrêtez temporairement lorsque vous recevez d'autres erreurs\n",
    "        except tweepy.TweepError as e:\n",
    "            if 'Failed to send request:' in e.reason:\n",
    "                print(\"Time out error caught.\")\n",
    "                time.sleep(1001)\n",
    "                search_tweets(search_words, date_since)\n",
    "            elif'Too Many Requests' in e.reason:\n",
    "                print(\"Too many requests, sleeping for 15 min\")\n",
    "                time.sleep(1001)\n",
    "                search_tweets(search_words, date_since)\n",
    "            else:\n",
    "                print(e)\n",
    "                print(\"Other error with this user...passing\")\n",
    "                pass\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    work()\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 2 : Prétraitement des tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "il faut tout d'abord collecter les trois fichiers qu'on a créer : \n",
    "    \n",
    "    extractionNews1Twitter.csv\n",
    "    extractionNews2Twitter.csv\n",
    "    extractionNews3Twitter.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Sauvgarder les tweets dans une dataframe :\n",
    "### 2.1.1)  1ere dataframe ***03/12/2020***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "executionInfo": {
     "elapsed": 667,
     "status": "ok",
     "timestamp": 1607551471060,
     "user": {
      "displayName": "Amina Sridi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfTzr62TA1NJlnTLYck_lihI2hy50RTcOM1aYznQ=s64",
      "userId": "03842942722047736776"
     },
     "user_tz": -60
    },
    "id": "MQ9_sH_A-aih",
    "outputId": "8df56293-37d7-40ed-d716-6e7968b636bf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Contenu_</th>\n",
       "      <th>Date_</th>\n",
       "      <th>Utilisateur_</th>\n",
       "      <th>Source_</th>\n",
       "      <th>ID_</th>\n",
       "      <th>Tweet URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>White House communications director Alyssa Far...</td>\n",
       "      <td>2020-12-03 22:19:11</td>\n",
       "      <td>PTekach</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>1334623241416151040</td>\n",
       "      <td>https://twitter.com/statuses/1334623241416151040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sen. Loeffler made large donations to anti-abo...</td>\n",
       "      <td>2020-12-03 22:19:09</td>\n",
       "      <td>chemasilva123</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>1334623233178558464</td>\n",
       "      <td>https://twitter.com/statuses/1334623233178558464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fake news, Barth na biso. Le jour viendra où c...</td>\n",
       "      <td>2020-12-03 22:19:05</td>\n",
       "      <td>Mutombolaurent</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>1334623218016063488</td>\n",
       "      <td>https://twitter.com/statuses/1334623218016063488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L'homme d'affaires était... à Barcelone\\r\\n\\r\\...</td>\n",
       "      <td>2020-12-03 22:18:40</td>\n",
       "      <td>YahooPopFR</td>\n",
       "      <td>TweetDeck</td>\n",
       "      <td>1334623111640125441</td>\n",
       "      <td>https://twitter.com/statuses/1334623111640125441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Consultation Développement soutenable 2020-202...</td>\n",
       "      <td>2020-12-03 22:17:16</td>\n",
       "      <td>LifeSciencesUPS</td>\n",
       "      <td>Scoop.it</td>\n",
       "      <td>1334622757632569344</td>\n",
       "      <td>https://twitter.com/statuses/1334622757632569344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Contenu_                Date_  \\\n",
       "0  White House communications director Alyssa Far...  2020-12-03 22:19:11   \n",
       "1  Sen. Loeffler made large donations to anti-abo...  2020-12-03 22:19:09   \n",
       "2  Fake news, Barth na biso. Le jour viendra où c...  2020-12-03 22:19:05   \n",
       "3  L'homme d'affaires était... à Barcelone\\r\\n\\r\\...  2020-12-03 22:18:40   \n",
       "4  Consultation Développement soutenable 2020-202...  2020-12-03 22:17:16   \n",
       "\n",
       "      Utilisateur_              Source_                  ID_  \\\n",
       "0          PTekach  Twitter for Android  1334623241416151040   \n",
       "1    chemasilva123      Twitter Web App  1334623233178558464   \n",
       "2   Mutombolaurent      Twitter Web App  1334623218016063488   \n",
       "3       YahooPopFR            TweetDeck  1334623111640125441   \n",
       "4  LifeSciencesUPS             Scoop.it  1334622757632569344   \n",
       "\n",
       "                                          Tweet URL  \n",
       "0  https://twitter.com/statuses/1334623241416151040  \n",
       "1  https://twitter.com/statuses/1334623233178558464  \n",
       "2  https://twitter.com/statuses/1334623218016063488  \n",
       "3  https://twitter.com/statuses/1334623111640125441  \n",
       "4  https://twitter.com/statuses/1334622757632569344  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "données1 = pd.read_csv ('extractionNews1Twitter.csv',encoding= 'unicode_escape') \n",
    "données1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1099,
     "status": "ok",
     "timestamp": 1607428290688,
     "user": {
      "displayName": "Amina Sridi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfTzr62TA1NJlnTLYck_lihI2hy50RTcOM1aYznQ=s64",
      "userId": "03842942722047736776"
     },
     "user_tz": -60
    },
    "id": "XDpWWPjR-aii",
    "outputId": "b5f4baa9-0e9e-44b9-d9bf-5ceca2db6f0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3162, 6)\n"
     ]
    }
   ],
   "source": [
    "print(données1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2) 2eme dataframe ***07/12/2020***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 947
    },
    "executionInfo": {
     "elapsed": 768,
     "status": "ok",
     "timestamp": 1607428294377,
     "user": {
      "displayName": "Amina Sridi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfTzr62TA1NJlnTLYck_lihI2hy50RTcOM1aYznQ=s64",
      "userId": "03842942722047736776"
     },
     "user_tz": -60
    },
    "id": "eu_w7YftF5cK",
    "outputId": "6a2ab583-8dc2-4ff9-acba-13a80c288d50"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Contenu_</th>\n",
       "      <th>Date_</th>\n",
       "      <th>Utilisateur_</th>\n",
       "      <th>Source_</th>\n",
       "      <th>ID_</th>\n",
       "      <th>Tweet URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lesbians, man.\\r\\n\\r\\nhttps://t.co/3A86nx0FAA</td>\n",
       "      <td>2020-12-07 21:02:55</td>\n",
       "      <td>Civil_Beaver</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>1336053598992674817</td>\n",
       "      <td>https://twitter.com/statuses/1336053598992674817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actualités - AR - Après deux nouveaux crimes, ...</td>\n",
       "      <td>2020-12-07 21:02:36</td>\n",
       "      <td>ebenesport</td>\n",
       "      <td>ebenesport autopublish</td>\n",
       "      <td>1336053521184157696</td>\n",
       "      <td>https://twitter.com/statuses/1336053521184157696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Le président Tebboune serait en Algérie ce soi...</td>\n",
       "      <td>2020-12-07 21:02:29</td>\n",
       "      <td>Dmounir6</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>1336053488632131585</td>\n",
       "      <td>https://twitter.com/statuses/1336053488632131585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Actualités - AR - La Suède et la Grèce dans le...</td>\n",
       "      <td>2020-12-07 21:02:27</td>\n",
       "      <td>ebenesport</td>\n",
       "      <td>ebenesport autopublish</td>\n",
       "      <td>1336053483703775233</td>\n",
       "      <td>https://twitter.com/statuses/1336053483703775233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Workyz_ Comment ça pour lutter contre les fak...</td>\n",
       "      <td>2020-12-07 21:02:24</td>\n",
       "      <td>blmawo</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>1336053470609149952</td>\n",
       "      <td>https://twitter.com/statuses/1336053470609149952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Contenu_                Date_  \\\n",
       "0      Lesbians, man.\\r\\n\\r\\nhttps://t.co/3A86nx0FAA  2020-12-07 21:02:55   \n",
       "1  Actualités - AR - Après deux nouveaux crimes, ...  2020-12-07 21:02:36   \n",
       "2  Le président Tebboune serait en Algérie ce soi...  2020-12-07 21:02:29   \n",
       "3  Actualités - AR - La Suède et la Grèce dans le...  2020-12-07 21:02:27   \n",
       "4  @Workyz_ Comment ça pour lutter contre les fak...  2020-12-07 21:02:24   \n",
       "\n",
       "   Utilisateur_                 Source_                  ID_  \\\n",
       "0  Civil_Beaver     Twitter for Android  1336053598992674817   \n",
       "1    ebenesport  ebenesport autopublish  1336053521184157696   \n",
       "2      Dmounir6     Twitter for Android  1336053488632131585   \n",
       "3    ebenesport  ebenesport autopublish  1336053483703775233   \n",
       "4        blmawo     Twitter for Android  1336053470609149952   \n",
       "\n",
       "                                          Tweet URL  \n",
       "0  https://twitter.com/statuses/1336053598992674817  \n",
       "1  https://twitter.com/statuses/1336053521184157696  \n",
       "2  https://twitter.com/statuses/1336053488632131585  \n",
       "3  https://twitter.com/statuses/1336053483703775233  \n",
       "4  https://twitter.com/statuses/1336053470609149952  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "données2 = pd.read_csv ('extractionNews2Twitter.csv',encoding= 'unicode_escape') \n",
    "données2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 743,
     "status": "ok",
     "timestamp": 1607428298916,
     "user": {
      "displayName": "Amina Sridi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfTzr62TA1NJlnTLYck_lihI2hy50RTcOM1aYznQ=s64",
      "userId": "03842942722047736776"
     },
     "user_tz": -60
    },
    "id": "COtDiSMzGF_Q",
    "outputId": "5c08407b-5008-448f-a26e-1130d2cf1d21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3237, 6)\n"
     ]
    }
   ],
   "source": [
    "print(données2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3)   3eme dataframe ***11/12/2020***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Contenu_</th>\n",
       "      <th>Date_</th>\n",
       "      <th>Utilisateur_</th>\n",
       "      <th>Source_</th>\n",
       "      <th>ID_</th>\n",
       "      <th>Tweet URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ford livre des repas en voitures autonomes à M...</td>\n",
       "      <td>2020-12-11 16:20:44</td>\n",
       "      <td>Alorscaroule</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>1337432137956548609</td>\n",
       "      <td>https://twitter.com/statuses/1337432137956548609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MERCATO: Limmense défi de Messi en Italie ? -...</td>\n",
       "      <td>2020-12-11 16:20:42</td>\n",
       "      <td>ebenesport</td>\n",
       "      <td>ebenesport autopublish</td>\n",
       "      <td>1337432130905845760</td>\n",
       "      <td>https://twitter.com/statuses/1337432130905845760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Le jeu de stratégie et de gestion en bac à sab...</td>\n",
       "      <td>2020-12-11 16:20:37</td>\n",
       "      <td>N_Difference</td>\n",
       "      <td>TweetDeck</td>\n",
       "      <td>1337432109619814402</td>\n",
       "      <td>https://twitter.com/statuses/1337432109619814402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eddie Van Halen fait partie des personnes les ...</td>\n",
       "      <td>2020-12-11 16:20:24</td>\n",
       "      <td>MetalZoneFR</td>\n",
       "      <td>IFTTT</td>\n",
       "      <td>1337432055085461505</td>\n",
       "      <td>https://twitter.com/statuses/1337432055085461505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Votez pour le meilleur album Metal/Rock de l'a...</td>\n",
       "      <td>2020-12-11 16:20:20</td>\n",
       "      <td>MetalZoneFR</td>\n",
       "      <td>IFTTT</td>\n",
       "      <td>1337432035993022464</td>\n",
       "      <td>https://twitter.com/statuses/1337432035993022464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Contenu_                Date_  \\\n",
       "0  Ford livre des repas en voitures autonomes à M...  2020-12-11 16:20:44   \n",
       "1  MERCATO: Limmense défi de Messi en Italie ? -...  2020-12-11 16:20:42   \n",
       "2  Le jeu de stratégie et de gestion en bac à sab...  2020-12-11 16:20:37   \n",
       "3  Eddie Van Halen fait partie des personnes les ...  2020-12-11 16:20:24   \n",
       "4  Votez pour le meilleur album Metal/Rock de l'a...  2020-12-11 16:20:20   \n",
       "\n",
       "   Utilisateur_                 Source_                  ID_  \\\n",
       "0  Alorscaroule     Twitter for Android  1337432137956548609   \n",
       "1    ebenesport  ebenesport autopublish  1337432130905845760   \n",
       "2  N_Difference               TweetDeck  1337432109619814402   \n",
       "3   MetalZoneFR                   IFTTT  1337432055085461505   \n",
       "4   MetalZoneFR                   IFTTT  1337432035993022464   \n",
       "\n",
       "                                          Tweet URL  \n",
       "0  https://twitter.com/statuses/1337432137956548609  \n",
       "1  https://twitter.com/statuses/1337432130905845760  \n",
       "2  https://twitter.com/statuses/1337432109619814402  \n",
       "3  https://twitter.com/statuses/1337432055085461505  \n",
       "4  https://twitter.com/statuses/1337432035993022464  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "données3 = pd.read_csv ('extractionNews3Twitter.csv',encoding= 'unicode_escape') \n",
    "données3.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3125, 6)\n"
     ]
    }
   ],
   "source": [
    "print(données3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBcD1oRYfp5Q"
   },
   "source": [
    "###  2.1.4) Concatiner les trois dataFrames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 670,
     "status": "ok",
     "timestamp": 1607432124864,
     "user": {
      "displayName": "Amina Sridi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfTzr62TA1NJlnTLYck_lihI2hy50RTcOM1aYznQ=s64",
      "userId": "03842942722047736776"
     },
     "user_tz": -60
    },
    "id": "ntqe9I2lGrch",
    "outputId": "34ea6581-4684-4815-adaa-9d4780e831ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9524, 6)\n"
     ]
    }
   ],
   "source": [
    "df_merge_col = pd.concat([données3,données1, données2 ])\n",
    "print(df_merge_col.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 908,
     "status": "ok",
     "timestamp": 1607432128011,
     "user": {
      "displayName": "Amina Sridi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfTzr62TA1NJlnTLYck_lihI2hy50RTcOM1aYznQ=s64",
      "userId": "03842942722047736776"
     },
     "user_tz": -60
    },
    "id": "qgKC6RD2CK3s",
    "outputId": "5e826837-e1de-4e9c-b873-db891fa2d2f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9524 entries, 0 to 3236\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Contenu_      9524 non-null   object\n",
      " 1   Date_         9524 non-null   object\n",
      " 2   Utilisateur_  9524 non-null   object\n",
      " 3   Source_       9507 non-null   object\n",
      " 4   ID_           9524 non-null   int64 \n",
      " 5   Tweet URL     9524 non-null   object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 520.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_merge_col.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on va s'interesser juste sur la colonne **Contenu_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VT-1AS1kdKm"
   },
   "source": [
    "## 2.2) Prétraitement des tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette étape, l’objectif est d’éliminer le texte inutile des tweets tels que les #, les noms des utilisateurs, les url, …\n",
    "\n",
    "Le nettoyage des tweets comprendra plusieurs choses \n",
    "\n",
    "pour ne pas se tromper il vaut mieux aller du plus restrictif au moins restrictif.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1) Eliminer les données unitilies : nom-utilisateur, URL..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "executionInfo": {
     "elapsed": 629,
     "status": "ok",
     "timestamp": 1607432133848,
     "user": {
      "displayName": "Amina Sridi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfTzr62TA1NJlnTLYck_lihI2hy50RTcOM1aYznQ=s64",
      "userId": "03842942722047736776"
     },
     "user_tz": -60
    },
    "id": "0iTcxxPu_EIs",
    "outputId": "4203eab4-1daa-453e-8156-0a55d6f7a5f1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Contenu_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ford livre des repas en voitures autonomes à M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MERCATO: Limmense défi de Messi en Italie ? -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Le jeu de stratégie et de gestion en bac à sab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eddie Van Halen fait partie des personnes les ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Votez pour le meilleur album Metal/Rock de l'a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ella's Absence Seizures Explained by Ella  EP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The game awards, les annonces - Shady Part of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Cyberpunk 2077 : oups, des pénis sortent des p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>J'aime beaucoup les frères #Tadros ( #DigitalW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Persona 5 Strikers : On a vu la suite de Perso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"Felicity Machado\" est ma 1093 ième source d'a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Un tournevis semble l'outil le plus simple au ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Un sénateur républicain blanc et solitaire blo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@EmmanuelMacron \\r\\n#ViolencesPolicières \\r\\nM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@VirusWar Merci pour la News !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@resilie28575107 Donc ce n'est jamais arrivé.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>#rclens #RCLMHSC  | Lens - Montpellier: Le gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>L'IA pourrait remplacer l'Homme dans le secteu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Depuis un moment\\r\\nIl semblerait qu'il fasse ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ultra-gauche : sept personnes soupçonnées d'av...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Contenu_\n",
       "0   Ford livre des repas en voitures autonomes à M...\n",
       "1   MERCATO: Limmense défi de Messi en Italie ? -...\n",
       "2   Le jeu de stratégie et de gestion en bac à sab...\n",
       "3   Eddie Van Halen fait partie des personnes les ...\n",
       "4   Votez pour le meilleur album Metal/Rock de l'a...\n",
       "5   Ella's Absence Seizures Explained by Ella  EP...\n",
       "6   The game awards, les annonces - Shady Part of ...\n",
       "7   Cyberpunk 2077 : oups, des pénis sortent des p...\n",
       "8   J'aime beaucoup les frères #Tadros ( #DigitalW...\n",
       "9   Persona 5 Strikers : On a vu la suite de Perso...\n",
       "10  \"Felicity Machado\" est ma 1093 ième source d'a...\n",
       "11  Un tournevis semble l'outil le plus simple au ...\n",
       "12  Un sénateur républicain blanc et solitaire blo...\n",
       "13  @EmmanuelMacron \\r\\n#ViolencesPolicières \\r\\nM...\n",
       "14                     @VirusWar Merci pour la News !\n",
       "15  @resilie28575107 Donc ce n'est jamais arrivé.....\n",
       "16  #rclens #RCLMHSC  | Lens - Montpellier: Le gro...\n",
       "17  L'IA pourrait remplacer l'Homme dans le secteu...\n",
       "18  Depuis un moment\\r\\nIl semblerait qu'il fasse ...\n",
       "19  Ultra-gauche : sept personnes soupçonnées d'av..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete=[\"Tweet URL\",\"ID_\",\"Source_\",\"Utilisateur_\",\"Date_\"]\n",
    "alldata = df_merge_col.drop(delete, axis=1)\n",
    "alldata.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2) Eliminer les les caractéres non utiles : #, @ ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà à quoi ressemble notre pipeline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "SbU9B5ZZPPuT"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def nlp_pipeline(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "    text = ' '.join(text.split())\n",
    "    text = re.sub(r\"[A-Za-z\\.]*[0-9]+[A-Za-z%°\\.]*\", \"\", text)\n",
    "    text = re.sub(r\"(\\s\\-\\s|-$)\", \"\", text)\n",
    "    text = re.sub(r\"[,\\!\\?\\%\\(\\)\\/\\\"]\", \"\", text)\n",
    "    text = re.sub(r\"\\&\\S*\\s\", \"\", text)\n",
    "    text = re.sub(r\"\\&\", \"\", text)\n",
    "    text = re.sub(r\"\\+\", \"\", text)\n",
    "    text = re.sub(r\"\\#\", \"\", text)\n",
    "    text = re.sub(r\"\\$\", \"\", text)\n",
    "    text = re.sub(r\"\\£\", \"\", text)\n",
    "    text = re.sub(r\"\\%\", \"\", text)\n",
    "    text = re.sub(r\"\\:\", \"\", text)\n",
    "    text = re.sub(r\"\\@\", \"\", text)\n",
    "    text = re.sub(r\"\\-\", \"\", text)\n",
    "    text = re.sub(r\"\\Ã\", \"\", text)\n",
    "    text = re.sub(r\"\\©\", \"\", text)\n",
    "    \n",
    "    \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce pipeline nous permet d’avoir des tweets à peu prés propres\n",
    "\n",
    "on va l'appliquer sur la colonne`Contenu_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "34HzLHkbPUjC"
   },
   "outputs": [],
   "source": [
    "corpus = alldata['Contenu_']\n",
    "corpus_clean = corpus.apply(nlp_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 680,
     "status": "ok",
     "timestamp": 1607432165383,
     "user": {
      "displayName": "Amina Sridi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfTzr62TA1NJlnTLYck_lihI2hy50RTcOM1aYznQ=s64",
      "userId": "03842942722047736776"
     },
     "user_tz": -60
    },
    "id": "lJ-ZuC2rQCHz",
    "outputId": "874df17a-ce6f-47f3-875d-4bb5af4f7edf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ford livre des repas en voitures autonomes à m...\n",
       "1     mercato limmense défi de messi en italie http...\n",
       "2     le jeu de stratégie et de gestion en bac à sab...\n",
       "3     eddie van halen fait partie des personnes les ...\n",
       "4     votez pour le meilleur album metalrock de l'an...\n",
       "5     ella's absence seizures explained by ella  ep...\n",
       "6     the game awards les annoncesshady part of me e...\n",
       "7     cyberpunk   oups des pénis sortent des pantalo...\n",
       "8     j'aime beaucoup les frères tadros  digitalworl...\n",
       "9     persona  strikers  on a vu la suite de persona...\n",
       "10    felicity machado est ma  ième source d'actuali...\n",
       "11    un tournevis semble l'outil le plus simple au ...\n",
       "12    un sénateur républicain blanc et solitaire blo...\n",
       "13    emmanuelmacron violencespolicières manifestant...\n",
       "14                         viruswar merci pour la news \n",
       "15              donc ce n'est jamais arrivé.. fake news\n",
       "16    rclens rclmhsc | lensmontpellier le groupe len...\n",
       "17    l'ia pourrait remplacer l'homme dans le secteu...\n",
       "18    depuis un moment il semblerait qu'il fasse un ...\n",
       "19    ultragauche  sept personnes soupçonnées d'avoi...\n",
       "Name: Contenu_, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_clean.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3) sauvgarder les donner dans un fichier csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "1bEBiy3jNb3Y"
   },
   "outputs": [],
   "source": [
    "corpus_clean.to_csv(\"corpus_clean.csv\",index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "executionInfo": {
     "elapsed": 822,
     "status": "ok",
     "timestamp": 1607547656335,
     "user": {
      "displayName": "Amina Sridi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfTzr62TA1NJlnTLYck_lihI2hy50RTcOM1aYznQ=s64",
      "userId": "03842942722047736776"
     },
     "user_tz": -60
    },
    "id": "ol5am62XxJgd",
    "outputId": "a514595d-e7a5-4863-8f55-f251b17256e6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Contenu_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ford livre des repas en voitures autonomes à m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mercato limmense défi de messi en italie http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>le jeu de stratégie et de gestion en bac à sab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eddie van halen fait partie des personnes les ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>votez pour le meilleur album metalrock de l'an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ella's absence seizures explained by ella  ep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the game awards les annoncesshady part of me e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cyberpunk   oups des pénis sortent des pantalo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>j'aime beaucoup les frères tadros  digitalworl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>persona  strikers  on a vu la suite de persona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>felicity machado est ma  ième source d'actuali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>un tournevis semble l'outil le plus simple au ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>un sénateur républicain blanc et solitaire blo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>emmanuelmacron violencespolicières manifestant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>viruswar merci pour la news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>donc ce n'est jamais arrivé.. fake news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rclens rclmhsc | lensmontpellier le groupe len...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>l'ia pourrait remplacer l'homme dans le secteu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>depuis un moment il semblerait qu'il fasse un ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ultragauche  sept personnes soupçonnées d'avoi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Contenu_\n",
       "0   ford livre des repas en voitures autonomes à m...\n",
       "1   mercato limmense défi de messi en italie http...\n",
       "2   le jeu de stratégie et de gestion en bac à sab...\n",
       "3   eddie van halen fait partie des personnes les ...\n",
       "4   votez pour le meilleur album metalrock de l'an...\n",
       "5   ella's absence seizures explained by ella  ep...\n",
       "6   the game awards les annoncesshady part of me e...\n",
       "7   cyberpunk   oups des pénis sortent des pantalo...\n",
       "8   j'aime beaucoup les frères tadros  digitalworl...\n",
       "9   persona  strikers  on a vu la suite de persona...\n",
       "10  felicity machado est ma  ième source d'actuali...\n",
       "11  un tournevis semble l'outil le plus simple au ...\n",
       "12  un sénateur républicain blanc et solitaire blo...\n",
       "13  emmanuelmacron violencespolicières manifestant...\n",
       "14                       viruswar merci pour la news \n",
       "15            donc ce n'est jamais arrivé.. fake news\n",
       "16  rclens rclmhsc | lensmontpellier le groupe len...\n",
       "17  l'ia pourrait remplacer l'homme dans le secteu...\n",
       "18  depuis un moment il semblerait qu'il fasse un ...\n",
       "19  ultragauche  sept personnes soupçonnées d'avoi..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv ('corpus_clean.csv',encoding= 'utf-8') \n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "array=np.array(df)\n",
    "import pandas as pd \n",
    "dataframe=pd.DataFrame(array,columns=['Contenu_'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 3 : Traitement des tweets : NLP (Natural LanguageProcessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMYb1dgbvJ8l"
   },
   "source": [
    "## **Install NLTK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3506,
     "status": "ok",
     "timestamp": 1607548823831,
     "user": {
      "displayName": "Amina Sridi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfTzr62TA1NJlnTLYck_lihI2hy50RTcOM1aYznQ=s64",
      "userId": "03842942722047736776"
     },
     "user_tz": -60
    },
    "id": "RxMmeNb9TMyZ",
    "outputId": "9e21dd55-b6b7-4d79-f017-3f11658d0f37"
   },
   "source": [
    "`!pip3 install nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk.download()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8-TZ-4oYOep"
   },
   "source": [
    "## 3.1) Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> on peut appliquer la segmentation mes ca sert à rien puisque en utilie une Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "with open(\"corpus_clean.csv\") as f:\n",
    "    s=f.read()\n",
    "sents=(sent_tokenize(s))\n",
    "#print(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extracter tous les mots "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> on peut extracter tous les mots comme indique le code suivant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.tokenize import word_tokenize\n",
    "#with open(\"corpus_clean.csv\") as f:\n",
    "#    s=f.read()\n",
    "#words=word_tokenize(s)\n",
    "\n",
    "#print(words)\n",
    "\n",
    "#print (words[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mais on a besoins de tokenization de chaque tweet comme suivant : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformer chaque tweet en un ensemble de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token=dataframe\n",
    "df_token['Contenu_']=df_token['Contenu_'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9524, 1)\n"
     ]
    }
   ],
   "source": [
    "print(df_token.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Contenu_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ford, livre, des, repas, en, voitures, autono...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[mercato, limmense, défi, de, messi, en, ital...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[le, jeu, de, stratégie, et, de, gestion, en, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[eddie, van, halen, fait, partie, des, personn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[votez, pour, le, meilleur, album, metalrock, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ella, 's, absence, seizures, explained, by, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[the, game, awards, les, annoncesshady, part, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[cyberpunk, oups, des, pénis, sortent, des, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[j'aime, beaucoup, les, frères, tadros, digita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[persona, strikers, on, a, vu, la, suite, de, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Contenu_\n",
       "0  [ford, livre, des, repas, en, voitures, autono...\n",
       "1  [mercato, limmense, défi, de, messi, en, ital...\n",
       "2  [le, jeu, de, stratégie, et, de, gestion, en, ...\n",
       "3  [eddie, van, halen, fait, partie, des, personn...\n",
       "4  [votez, pour, le, meilleur, album, metalrock, ...\n",
       "5  [ella, 's, absence, seizures, explained, by, e...\n",
       "6  [the, game, awards, les, annoncesshady, part, ...\n",
       "7  [cyberpunk, oups, des, pénis, sortent, des, pa...\n",
       "8  [j'aime, beaucoup, les, frères, tadros, digita...\n",
       "9  [persona, strikers, on, a, vu, la, suite, de, ..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_token.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > notre data frame est transformée à une data frame des listes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3) lemmatisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk.download()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-1b55ba0c2d93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mword_lem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m\":\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mword_lem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "word_lem=WordNetLemmatizer()\n",
    "for word in words:\n",
    "    print(word +\":\" + word_lem.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4) Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk.download('stopwords')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "ps= PorterStemmer()\n",
    "stemmed_dataset=[]\n",
    "for i in range(0,9524):\n",
    "    stemmed_array = df['Contenu_'][i].split()\n",
    "    stemmed = [ps.stem(word) for word in stemmed_array if not word in set(stopwords.words('english'))]\n",
    "    stemmd =' '.join(stemmed)\n",
    "    stemmed_dataset.append(stemmed)\n",
    "print(stemmed_dataset[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_stem = pd.DataFrame(stemmed_dataset)\n",
    "df_stem.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_stem.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convertissez des phrases en vecteurs en utilisant TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord, nous devrons créer un modèle gensim pour convertir nos données textuelles en représentation vectorielle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforme le texte en une matrice clairsemée "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_clean.head(20)\n",
    "print(corpus_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=(corpus_clean.to_string())\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "counts = count_vect.fit_transform(corpus_clean)\n",
    "counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On aboutit à une matrice sparse ou chaque expression est représentée à une vecteur ou chaque 1 représente l’appartenance d’un mot à l’ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### td-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effectue la transformation TF-IDF à partir d'une matrice de comptage fournie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()\n",
    "res = tfidf.fit_transform(counts)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#res[0,:].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "res2 = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(res2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "wcss=[]\n",
    "\n",
    "for i in range(0,df.shape[0]):\n",
    "    kmeans =KMeans(n_clusters=3)\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=300, random_state=0, verbose=True)\n",
    "    kmeans.fit(res)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "4MiJq0zjuQUG"
   ],
   "name": "tweet.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
